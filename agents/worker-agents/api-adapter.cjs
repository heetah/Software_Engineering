/**
 * Cloud API Adapter
 * æ”¯æ´å¤šç¨® LLM API (OpenAI, Google Gemini ç­‰)
 */

async function callCloudAPI({ endpoint, apiKey, systemPrompt, userPrompt, maxTokens = 8192, modelTier = 'strong' }) {
  // ä½¿ç”¨ Node.js 18+ å…§å»º fetch æˆ– node-fetch
  let fetch;
  try {
    fetch = global.fetch || require('node-fetch');
  } catch {
    fetch = global.fetch;
  }

  // åµæ¸¬ API é¡žåž‹
  const isGemini = endpoint.includes('generativelanguage.googleapis.com');

  let requestBody, headers, apiUrl;

  if (isGemini) {
    // Google Gemini API æ ¼å¼
    apiUrl = `${endpoint}?key=${apiKey}`;

    // è‡ªé©æ‡‰æ¨¡åž‹é¸æ“‡ (Adaptive Model Selection)
    if (modelTier === 'fast' && endpoint.includes('/models/')) {
      // å˜—è©¦ä½¿ç”¨ Flash æ¨¡åž‹ (é€Ÿåº¦æ›´å¿«ï¼Œæˆæœ¬æ›´ä½Ž - é¡žæ¯”é‡åŒ–æ¨¡åž‹)
      apiUrl = apiUrl.replace(/\/models\/[^:]+:/, '/models/gemini-1.5-flash:');
      console.log(`[API Adapter] âš¡ Using FAST model (Gemini Flash)`);
    } else if (modelTier === 'strong' && endpoint.includes('/models/')) {
      // Enforce Pro model for strong tier
      apiUrl = apiUrl.replace(/\/models\/[^:]+:/, '/models/gemini-1.5-pro:');
      console.log(`[API Adapter] ðŸ§  Using STRONG model (Gemini Pro)`);
    }

    headers = { 'Content-Type': 'application/json' };
    requestBody = {
      contents: [{
        parts: [{
          text: `${systemPrompt}\n\n${userPrompt}`
        }]
      }],
      generationConfig: {
        temperature: 0.7,
        maxOutputTokens: maxTokens
      }
    };
  } else {
    // OpenAI API æ ¼å¼ (GPT-4, Azure OpenAI ç­‰)
    // è‡ªé©æ‡‰æ¨¡åž‹é¸ (Adaptive Model Selection)
    let modelName = 'gpt-4o'; // Default strong
    if (modelTier === 'fast') {
      modelName = 'gpt-4o-mini';
      console.log(`[API Adapter] âš¡ Using FAST model (GPT-4o-mini)`);
    } else {
      console.log(`[API Adapter] Using STRONG model (${modelName})`);
    }

    apiUrl = endpoint;
    headers = {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    };
    requestBody = {
      model: modelName,
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      max_tokens: maxTokens,
      temperature: 0.7
    };
  }

  const response = await fetch(apiUrl, {
    method: 'POST',
    headers,
    body: JSON.stringify(requestBody)
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`API error: ${response.status} - ${errorText}`);
  }

  const data = await response.json();

  // è§£æžå›žæ‡‰
  let content, tokensUsed;

  if (isGemini) {
    const candidate = data.candidates?.[0];
    content = candidate?.content?.parts?.[0]?.text || '';
    tokensUsed = data.usageMetadata?.totalTokenCount || 0;

    // æª¢æŸ¥ Gemini æ˜¯å¦å› å®‰å…¨åŽŸå› é˜»æ­¢äº†å…§å®¹
    const finishReason = candidate?.finishReason;
    if (finishReason && finishReason !== 'STOP') {
      console.warn(`[API Adapter] Gemini finishReason: ${finishReason}`);
      if (finishReason === 'SAFETY' || finishReason === 'RECITATION') {
        throw new Error(`Content blocked by Gemini: ${finishReason}. Safety ratings: ${JSON.stringify(candidate?.safetyRatings || [])}`);
      }
      if (finishReason === 'MAX_TOKENS') {
        console.warn('[API Adapter] Response truncated due to MAX_TOKENS');
      }
    }

    if (!content && tokensUsed > 0) {
      console.warn('[API Adapter] Gemini returned no content despite consuming tokens:', {
        finishReason,
        tokensUsed,
        safetyRatings: candidate?.safetyRatings
      });
    }
  } else {
    content = data.choices?.[0]?.message?.content || '';
    tokensUsed = data.usage?.total_tokens || 0;
  }

  return { content, tokensUsed };
}

module.exports = { callCloudAPI };
